# -*- coding: utf-8 -*-
"""JelaJava.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t2fSf27BMOVTnLZi0fSC3rYtnsaefn-X

# **Building a Tourist Place Recommendation System**

## **Packages**
"""

import numpy as np # for numerical computations
import pandas as pd # for data analysis and manipulation

# for visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px 

# for modelling
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

"""# **Indonesian Tourism Destination Dataset**

Dataset Source : https://kaggle.com/aprabowo/indonesia-tourism-destination
"""

user = pd.read_csv ('https://raw.githubusercontent.com/deltadv/JelaJava/main/Machine%20Learning/Datasets/user.csv')
place = pd.read_csv ('https://raw.githubusercontent.com/deltadv/JelaJava/main/Machine%20Learning/Datasets/tourism_with_id.csv')
rating = pd.read_csv ('https://raw.githubusercontent.com/deltadv/JelaJava/main/Machine%20Learning/Datasets/tourism_rating.csv')

user.head()

place.head()

rating.head()

df_data = pd.merge(rating, user, on='User_Id')
df_data

"""**Displaying information of dataset**"""

user.info()

rating.info()

place.info()

"""### **Data Visualization**"""

plt.figure(figsize=(10, 10))

# Accumulating the values of each category using cumcount()
place['Count_Category'] = place.groupby('Category').cumcount()+1

# Creating a bar plot using seaborn
sorted_data = place.sort_values(["Count_Category"], ascending=False)
ax = sns.countplot(x='Category', data=sorted_data)

# Adding title and axis labels to the bar plot using seaborn
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Number of Destination Places by Category')

# Displaying the count above each bar.
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')

# Display the plot
plt.show()

# Calculating the total number of destinations in a city using the agg count function.
count_city = place.groupby('City')['Place_Name'].agg(['count'])

# Convert the result of aggregation into a pandas DataFrame
df_city_count = pd.DataFrame(count_city).reset_index()

# Display the resulting DataFrame
print(df_city_count)

plt.pie(df_city_count['count'],
        labels = df_city_count['City'],
        colors=['#02cecb', '#b4ffff', '#f8e16c', '#fed811', '#fdc100'],
        autopct='%1.1f%%',
        startangle=90)
plt.title('Percentage Ratio of Destination Count by City')

plt.show()

"""## **Data Preprocessing**"""

place.drop(['Time_Minutes','Coordinate','Lat','Long','Unnamed: 11','Unnamed: 12'],axis=1,inplace=True)
place

# Encode categorical features
df_data['User_Id'] = df_data['User_Id'].astype('category').cat.codes
df_data['Place_Id'] = df_data['Place_Id'].astype('category').cat.codes

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_data[['User_Id', 'Place_Id']], df_data['Place_Ratings'], test_size=0.2, random_state=42)

# Create user-item matrix
num_users = df_data['User_Id'].nunique()
num_places = df_data['Place_Id'].nunique()
user_item_matrix = np.zeros((num_users, num_places))
for index, row in df_data.iterrows():
    user_id = row['User_Id']
    place_id = row['Place_Id']
    rating = row['Place_Ratings']
    user_item_matrix[user_id, place_id] = rating

"""# **Modelling Data**"""

embedding_size = 32

# Define input layers
user_input = tf.keras.Input(shape=(1,))
place_input = tf.keras.Input(shape=(1,))

# Define embedding layers
user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)(user_input)
user_embedding = tf.keras.layers.Dropout(0.2)(user_embedding)  # Dropout regularization
place_embedding = tf.keras.layers.Embedding(num_places, embedding_size)(place_input)
place_embedding = tf.keras.layers.Dropout(0.2)(place_embedding)  # Dropout regularization

# Flatten embedding layers
user_embedding = tf.keras.layers.Flatten()(user_embedding)
place_embedding = tf.keras.layers.Flatten()(place_embedding)

# Compute dot product of user and place embeddings
dot_product = tf.keras.layers.Dot(axes=1)([user_embedding, place_embedding])

model = tf.keras.Model(inputs=[user_input, place_input], outputs=dot_product) # Create model
learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=10000, decay_rate=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(loss='mean_squared_error', optimizer='adam') # Compile model

# Define early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# Train model
epochs = 100
batch_size = 32

history = model.fit([X_train['User_Id'].values, X_train['Place_Id'].values], y_train.values,
                    validation_data=([X_test['User_Id'].values, X_test['Place_Id'].values], y_test.values),
                    batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])

"""### **Metrics Evaluation Data Training dan Testing**"""

# Visualize loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

# Generate Recommendations and Rating History for user
user_input = 5 #INPUT User_Id
user_id = user_input - 1

top_n = 5

# Get rating history user
user_ratings = user_item_matrix[user_id]

# Find unrated places
unrated_places = np.where(user_ratings == 0)[0]

# Generate recommendations using the trained model
user_ids = np.full_like(unrated_places, user_id)
recommendations = model.predict([user_ids, unrated_places])

# Get top N recommendations
top_indices = recommendations.flatten().argsort()[-top_n:][::-1]
top_places = unrated_places[top_indices]

# Get user's rating history for places with rating 5
rated_places = df_data[(df_data['User_Id'] == user_id) & (df_data['Place_Ratings'] == 5)]['Place_Id']
# Get place names with rating 5
rated_place_names = place.loc[rated_places, 'Place_Name'].values

# Get place names
place_names = place.loc[top_places, 'Place_Name'].values

print("Input ID user:", user_input)
print("User", user_input, "has visited the following places with a rating of 5:")
print("---")
for i, place_id in enumerate(rated_places):
    print(rated_place_names[i])

# Print Top 5 recommendations with place names
print("Top", top_n, "recommendations for User", user_input, ":")
print("---")
for i, place_id in enumerate(top_places):
    print(place_names[i])

# Retrieve all recommendation data for places
top_indices_all = recommendations.flatten().argsort()[::-1]
top_recommendations = unrated_places[top_indices_all]

print("List of All Recommendations Place for User", user_input, ":")
print("---")
place_names = place.loc[top_recommendations, 'Place_Name'].values
for i, place_id in enumerate(top_recommendations):
    print(place_names[i])

num_recommendations = len(top_recommendations)
print("Number of recommendations:", num_recommendations)

city = "Bandung"  # Replace with city names according to the user's choice

# Filtering Recommendations by City
filtered_recommendations = top_recommendations[place.loc[top_recommendations, 'City'] == city]
filtered_place_names = place.loc[filtered_recommendations, 'Place_Name'].values

print("Top recommendations for User", user_input, "in", city, ":")
print("---")
for i, place_id in enumerate(filtered_recommendations):
    print(filtered_place_names[i])

num_filtered_recommendations = len(filtered_recommendations)
print("Number of recommendations in", city, ":", num_filtered_recommendations)

from keras.models import model_from_json

# Simpan model ke dalam format JSON
model_json = model.to_json()
with open("JelaJava.json", "w") as json_file:
    json_file.write(model_json)

# Simpan bobot model ke dalam format H5
model.save("JelaJava.h5")

# !pip install tensorflow==1.15
# import tensorflow as tf
# from tensorflow.keras.models import load_model

# JelaJava = load_model('JelaJava.h5')
# sess = tf.compat.v1.keras.backend.get_session()
# tf.compat.v1.saved_model.simple_save(
#     sess,
#     'sample_data',
#     inputs={'input': JelaJava_h5.input},
#     outputs={t.name: t for t in JelaJava_h5.outputs}
# )

"""**Apabila ID user belum terdaftar**"""

# import numpy as np

# # Fungsi untuk memberikan rekomendasi kepada user
# def get_recommendations(user_id, top_n):
#     # Cek apakah user terdata di dataset
#     if user_id in df_data['User_Id'].unique():
#         # User terdata di dataset, gunakan pendekatan Content-Based Filtering
#         user_index = user_id - 1

#         # Get rating history user
#         user_ratings = user_item_matrix[user_index]

#         # Find unrated places
#         unrated_places = np.where(user_ratings == 0)[0]

#         # Generate recommendations using the trained model
#         user_ids = np.full_like(unrated_places, user_index)
#         recommendations = model.predict([user_ids, unrated_places])

#         # Get top N recommendations
#         top_indices = recommendations.flatten().argsort()[-top_n:][::-1]
#         top_places = unrated_places[top_indices]

#     else:
#         # User tidak terdata di dataset, berikan rekomendasi acak berdasarkan rating tertinggi
#         top_places = np.argsort(np.mean(user_item_matrix, axis=0))[::-1][:top_n]

#     # Get place names
#     place_names = place.loc[top_places, 'Place_Name'].values

#     return place_names

# # Contoh penggunaan
# user_id = 305  # ID user yang tidak terdata di dataset
# top_n = 5  # Jumlah rekomendasi yang diinginkan

# recommendations = get_recommendations(user_id, top_n)
# if len(recommendations) > 0:
#     print("Rekomendasi untuk User", user_id, ":")
#     for i, place_name in enumerate(recommendations):
#         print(place_name)
# else:
#     print("Tidak ada rekomendasi untuk User", user_id)