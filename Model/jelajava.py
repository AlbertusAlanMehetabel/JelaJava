# -*- coding: utf-8 -*-
"""JelaJava.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hp5uzrSgi07V7y6NojHUfwhBrZdys8Ok

# **Building a Tourist Place Recommendation System**

## **Packages**
"""

import numpy as np # for numerical computations
import pandas as pd # for data analysis and manipulation

# for visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# for modelling
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

"""## **Indonesian Tourism Destination Dataset**

Dataset Source : https://kaggle.com/aprabowo/indonesia-tourism-destination
"""

user = pd.read_csv ('https://raw.githubusercontent.com/AlbertusAlanMehetabel/JelaJava/machine-learning/Dataset/user.csv')
place = pd.read_csv ('https://raw.githubusercontent.com/AlbertusAlanMehetabel/JelaJava/machine-learning/Dataset/tourism_with_id.csv')
rating = pd.read_csv ('https://raw.githubusercontent.com/AlbertusAlanMehetabel/JelaJava/machine-learning/Dataset/tourism_rating.csv')

user.head()

place.head()

rating.head()

df_data = pd.merge(rating, user, on='User_Id')
df_data

"""**Displaying information of dataset**"""

user.info()

rating.info()

place.info()

"""### **Data Visualization**"""

plt.figure(figsize=(10, 10))

# Accumulating the values of each category using cumcount()
place['Count_Category'] = place.groupby('Category').cumcount()+1

# Creating a bar plot using seaborn
sorted_data = place.sort_values(["Count_Category"], ascending=False)
ax = sns.countplot(x='Category', data=sorted_data)

# Adding title and axis labels to the bar plot using seaborn
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Number of Destination Places by Category')

# Displaying the count above each bar.
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')

# Display the plot
plt.show()

# Calculating the total number of destinations in a city using the agg count function.
count_city = place.groupby('City')['Place_Name'].agg(['count'])

# Convert the result of aggregation into a pandas DataFrame
df_city_count = pd.DataFrame(count_city).reset_index()

# Display the resulting DataFrame
print(df_city_count)

plt.pie(df_city_count['count'],
        labels = df_city_count['City'],
        colors=['#02cecb', '#b4ffff', '#f8e16c', '#fed811', '#fdc100'],
        autopct='%1.1f%%',
        startangle=90)
plt.title('Percentage Ratio of Destination Count by City')

plt.show()

"""## **Data Preprocessing**"""

place.drop(['Time_Minutes','Coordinate','Lat','Long','Unnamed: 11','Unnamed: 12'],axis=1,inplace=True)
place

# Encode categorical features
df_data['User_Id'] = df_data['User_Id'].astype('category').cat.codes
df_data['Place_Id'] = df_data['Place_Id'].astype('category').cat.codes

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_data[['User_Id', 'Place_Id']], df_data['Place_Ratings'], test_size=0.2, random_state=42)

# Create user-item matrix
num_users = df_data['User_Id'].nunique()
num_places = df_data['Place_Id'].nunique()
user_item_matrix = np.zeros((num_users, num_places))
for index, row in df_data.iterrows():
    user_id = row['User_Id']
    place_id = row['Place_Id']
    rating = row['Place_Ratings']
    user_item_matrix[user_id, place_id] = rating

"""## **Data Modelling**"""

embedding_size = 32

# Define input layers
user_input = tf.keras.Input(shape=(1,))
place_input = tf.keras.Input(shape=(1,))

# Define embedding layers
user_embedding = tf.keras.layers.Embedding(num_users, embedding_size)(user_input)
user_embedding = tf.keras.layers.Dropout(0.2)(user_embedding)  # Dropout regularization
place_embedding = tf.keras.layers.Embedding(num_places, embedding_size)(place_input)
place_embedding = tf.keras.layers.Dropout(0.2)(place_embedding)  # Dropout regularization

# Flatten embedding layers
user_embedding = tf.keras.layers.Flatten()(user_embedding)
place_embedding = tf.keras.layers.Flatten()(place_embedding)

# Compute dot product of user and place embeddings
dot_product = tf.keras.layers.Dot(axes=1)([user_embedding, place_embedding])

model = tf.keras.Model(inputs=[user_input, place_input], outputs=dot_product)

# Compile model with appropriate loss function and optimizer
learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=10000, decay_rate=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(loss='mean_squared_error', optimizer=optimizer)

# Define early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# Train model
epochs = 100
batch_size = 32

history = model.fit(
    [X_train['User_Id'].values, X_train['Place_Id'].values],
    y_train.values,
    validation_data=([X_test['User_Id'].values, X_test['Place_Id'].values], y_test.values),
    batch_size=batch_size,
    epochs=epochs,
    callbacks=[early_stopping]
)

"""## **Models Evaluation Data Training dan Testing**"""

# Retrieve loss history
loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot loss curves
plt.plot(range(1, len(loss) + 1), loss, label='Training Loss')
plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""## **Model Deployment**"""

# Generate Recommendations and Rating History for user
user_input = 1 #INPUT User_Id
user_id = user_input - 1

# Get rating history user
user_ratings = user_item_matrix[user_id]

# Find unrated places
unrated_places = np.where(user_ratings == 0)[0]

# Generate recommendations using the trained model
user_ids = np.full_like(unrated_places, user_id)
input = [user_ids, unrated_places]
recommendations = model.predict(input)

# Flattening the recommendation results into one dimension
top_indices = np.argsort(recommendations.flatten())
top_recommendations = unrated_places[top_indices]

# List of All Recommendations Place for User
place_ids = []
place_names = []
cities = []
predictions = []
for i, place_id in enumerate(top_recommendations):
    place_ids.append(place.loc[place_id, 'Place_Id'])
    place_names.append(place.loc[place_id, 'Place_Name'])
    cities.append(place.loc[place_id, 'City'])
    predictions.append(recommendations.flatten()[i])

# Get user's rating history for places with rating 5
rated_places = df_data[(df_data['User_Id'] == user_id) & (df_data['Place_Ratings'] == 5)]['Place_Id']
# Get place names with rating 5
rated_place_names = place.loc[rated_places, 'Place_Name'].values

print("Input ID user:", user_input)
print("User", user_input, "has visited the following places with a rating of 5:")
print("---")
for i, place_id in enumerate(rated_places):
    print(rated_place_names[i])

# Create a DataFrame from the recommendations
recommendations_df = pd.DataFrame({'Place_Id': place_ids, 'Place_Name': place_names, 'City': cities, 'Prediction': predictions})
recommendations_df = recommendations_df.sort_values('Prediction', ascending=False)
recommendations_df.drop(['Prediction'], axis = 1, inplace=True)
print(recommendations_df)

num_recommendations = len(top_recommendations)
print("Number of recommendations:", num_recommendations)

# from keras.models import model_from_json

# # Simpan model ke dalam format JSON
# model_json = model.to_json()
# with open("JelaJava.json", "w") as json_file:
#     json_file.write(model_json)

# # Simpan bobot model ke dalam format H5
# model.save("JelaJava.h5")